# -*- coding: utf-8 -*-
"""Customer Segmentation.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1u-qN8Tl4_PWWALWsPM1hSCsz0QBXkdMX
"""

import numpy as np
import pandas as pd
import datetime
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from yellowbrick.cluster import KElbowVisualizer
#KElbowVisualizer is used to determine the optimal number of clusters in K-Means by visualizing the elbow point using distortion (Distortion (Inertia) → sum of squared distances from points to cluster center) or silhouette scores(Silhouette Score measures how well a data point fits within its assigned cluster compared to other clusters.).
from sklearn.cluster import KMeans
from mpl_toolkits.mplot3d import Axes3D  #from mpl_toolkits.mplot3d import Axes3D
from sklearn.cluster import AgglomerativeClustering
from sklearn import metrics
import sys
import warnings
if not sys.warnoptions:
    warnings.simplefilter("ignore")
np.random.seed(42)
#This code suppresses warning messages and sets a fixed random seed so that results are consistent every time the program is run.

data = pd.read_csv("/content/marketing_campaign.csv", sep="\t")
print("Number of datapoints:", len(data))
data.head()

print(data.info())

"""From the above output, we can conclude and note that:

There are missing values in income
Dt_Customer that indicates the date a customer joined the database is not parsed as DateTime
There are some categorical features in our data frame; as there are some features in dtype: object). So we will need to encode them into numeric forms later.
First of all, for the missing values, I am simply going to drop the rows that have missing income values.
"""

#To remove the NA values
data = data.dropna()
print("The total number of data-points:", len(data))

"""In the next step, I am going to create a feature out of "Dt_Customer" that indicates the number of days a customer is registered in the firm's database. However, in order to keep it simple, I am taking this value relative to the most recent customer in the record.

Thus to get the values I must check the newest and oldest recorded dates.
"""

data['Dt_Customer']=pd.to_datetime(data['Dt_Customer'],  dayfirst=True)
dates=[]
for i in data['Dt_Customer']:
  i=i.date()
  dates.append(i)
print("The newest customer's enrolment date in therecords:",max(dates))
print("The oldest customer's enrolment date in the records:",min(dates))

"""Creating a feature ("Customer_For") of the number of days the customers started to shop in the store relative to the last recorded date"""

#Created a feature "Customer_For"
days = []
d1 = max(dates) #taking it to be the newest customer
for i in dates:
    delta = d1 - i
    days.append(delta)
data["Customer_For"] = days
data["Customer_For"] = pd.to_numeric(data["Customer_For"], errors="coerce")

"""Now we will be exploring the unique values in the categorical features to get a clear idea of the data.


"""

print("Total categories in the feature Marital_Status:\n", data["Marital_Status"].value_counts(), "\n")
print("Total categories in the feature Education:\n", data["Education"].value_counts())

"""In the next bit, I will be performing the following steps to engineer some new features:

Extract the "Age" of a customer by the "Year_Birth" indicating the birth year of the respective person.
Create another feature "Spent" indicating the total amount spent by the customer in various categories over the span of two years.
Create another feature "Living_With" out of "Marital_Status" to extract the living situation of couples.
Create a feature "Children" to indicate total children in a household that is, kids and teenagers.
To get further clarity of household, Creating feature indicating "Family_Size"
Create a feature "Is_Parent" to indicate parenthood status
Lastly, I will create three categories in the "Education" by simplifying its value counts.
Dropping some of the redundant features
"""

#Feature Engineering
#Age of customer today
data["Age"] = 2021-data["Year_Birth"]

#Total spendings on various items
data["Spent"] = data["MntWines"]+ data["MntFruits"]+ data["MntMeatProducts"]+ data["MntFishProducts"]+ data["MntSweetProducts"]+ data["MntGoldProds"]

#Deriving living situation by marital status"Alone"
data["Living_With"]=data["Marital_Status"].replace({"Married":"Partner", "Together":"Partner", "Absurd":"Alone", "Widow":"Alone", "YOLO":"Alone", "Divorced":"Alone", "Single":"Alone",})

#Feature indicating total children living in the household
data["Children"]=data["Kidhome"]+data["Teenhome"]

#Feature for total members in the householde
data["Family_Size"] = data["Living_With"].replace({"Alone": 1, "Partner":2})+ data["Children"]

#Feature pertaining parenthood
data["Is_Parent"] = np.where(data.Children> 0, 1, 0)

#Segmenting education levels in three groups
data["Education"]=data["Education"].replace({"Basic":"Undergraduate","2n Cycle":"Undergraduate", "Graduation":"Graduate", "Master":"Postgraduate", "PhD":"Postgraduate"})

#For clarity
data=data.rename(columns={"MntWines": "Wines","MntFruits":"Fruits","MntMeatProducts":"Meat","MntFishProducts":"Fish","MntSweetProducts":"Sweets","MntGoldProds":"Gold"})

#Dropping some of the redundant features
to_drop = ["Marital_Status", "Dt_Customer", "Z_CostContact", "Z_Revenue", "Year_Birth", "ID"]
data = data.drop(to_drop, axis=1)

data.describe()

"""The above stats show some discrepancies in mean Income and Age and max Income and age.

Do note that max-age is 128 years, As I calculated the age that would be today (i.e. 2021) and the data is old.

I must take a look at the broader view of the data. I will plot some of the selected features.
"""

#To plot some selected features
#Setting up colors prefrences
sns.set(rc={"axes.facecolor":"#FFF9ED","figure.facecolor":"#FFF9ED"})
pallet = ["#682F2F", "#9E726F", "#D6B2B1", "#B9C0C9", "#9F8A78", "#F3AB60"]
cmap = colors.ListedColormap(["#682F2F", "#9E726F", "#D6B2B1", "#B9C0C9", "#9F8A78", "#F3AB60"])
#Plotting following features
To_Plot = [ "Income", "Recency", "Customer_For", "Age", "Spent", "Is_Parent"]
print("Reletive Plot Of Some Selected Features: A Data Subset")
plt.figure()
sns.pairplot(data[To_Plot], hue= "Is_Parent",palette= (["#682F2F","#F3AB60"]))
#Taking hue
plt.show()

#Dropping the outliers by setting a cap on Age and income.
data = data[(data["Age"]<90)]
data = data[(data["Income"]<600000)]
print("The total number of data-points after removing the outliers are:", len(data))

#correlation matrix
corrmat = data.select_dtypes(include='number').corr()
plt.figure(figsize=(20,20))
sns.heatmap(corrmat,annot=True, cmap=cmap, center=0)

#Get list of categorical variables
s = (data.dtypes == 'object')
object_cols = list(s[s].index)

print("Categorical variables in the dataset:", object_cols)

#Label Encoding the object dtypes.
LE=LabelEncoder()
for i in object_cols:
    data[i]=data[[i]].apply(LE.fit_transform)

print("All features are now numerical")

#Creating a copy of data
ds = data.copy()
# creating a subset of dataframe by dropping the features on deals accepted and promotions
cols_del = ['AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'AcceptedCmp1','AcceptedCmp2', 'Complain', 'Response']
ds = ds.drop(cols_del, axis=1)
#Scaling
scaler = StandardScaler()
scaler.fit(ds)
scaled_ds = pd.DataFrame(scaler.transform(ds),columns= ds.columns )
print("All features are now scaled")

#Scaled data to be used for reducing the dimensionality
print("Dataframe to be used for further modelling:")
scaled_ds.head()

"""In this problem, there are many factors on the basis of which the final classification will be done. These factors are basically attributes or features. The higher the number of features, the harder it is to work with it. Many of these features are correlated, and hence redundant. This is why I will be performing dimensionality reduction on the selected features before putting them through a classifier.
Dimensionality reduction is the process of reducing the number of random variables under consideration, by obtaining a set of principal variables.

Principal component analysis (PCA) is a technique for reducing the dimensionality of such datasets, increasing interpretability but at the same time minimizing information loss.

Steps in this section:

Dimensionality reduction with PCA
Plotting the reduced dataframe
Dimensionality reduction with PCA

For this project, I will be reducing the dimensions to 3.

https://www.kaggle.com/code/karnikakapoor/customer-segmentation-clustering/notebook
"""

#Initiating PCA to reduce dimentions aka features to 3

pca=PCA(n_components=3)
pca.fit(scaled_ds)
pca.fit(scaled_ds)
PCA_ds=pd.DataFrame(pca.transform(scaled_ds),columns=(["col1","col2", "col3"]))
PCA_ds.describe().T

#A 3D Projection Of Data In The Reduced Dimension

x=PCA_ds['col1']
y=PCA_ds['col2']
z=PCA_ds['col3']
#To plot

fig=plt.figure(figsize=(10,8))
ax=fig.add_subplot(111,projection='3d')
ax.scatter(x,y,z, c='maroon', marker='o')
ax.set_title('A 3D Projection Of Data In The Reduce Dimension')
plt.show()

"""###CLUSTERING

Now that I have reduced the attributes to three dimensions, I will be performing clustering via Agglomerative clustering. Agglomerative clustering is a hierarchical clustering method. It involves merging examples until the desired number of clusters is achieved.

Steps involved in the Clustering

Elbow Method to determine the number of clusters to be formed
Clustering via Agglomerative Clustering
Examining the clusters formed via scatter plot
"""

# Quick examination of elbow method to find numbers of clusters to make.
print('Elbow Method to determine the number of clusters to be formed:')
Elbow_M = KElbowVisualizer(KMeans(), k=10)
Elbow_M.fit(PCA_ds)
Elbow_M.show()

"""The above cell indicates that four will be an optimal number of clusters for this data. Next, we will be fitting the Agglomerative Clustering Model to get the final clusters."""

#Initiating the Agglomerative Clustering model

AC=AgglomerativeClustering(n_clusters=4)
# fit model and predict clusters
ypred_AC=AC.fit_predict(PCA_ds)
PCA_ds['Clusters']=ypred_AC
#Adding the Clusters feature to the orignal dataframe.
data["Clusters"]= ypred_AC

"""To examine the clusters formed let's have a look at the 3-D distribution of the clusters."""

#Plotting the clusters
fig = plt.figure(figsize=(10,8))
ax=plt.subplot(111,projection='3d',label='bla')
ax.scatter(x,y,z,s=40,c=PCA_ds['Clusters'],marker='o',cmap=cmap)
ax.set_title("The Plot Of The Clusters")
plt.show()

"""#EVALUATING MODELS

Since this is an unsupervised clustering. We do not have a tagged feature to evaluate or score our model. The purpose of this section is to study the patterns in the clusters formed and determine the nature of the clusters' patterns.

For that, we will be having a look at the data in light of clusters via exploratory data analysis and drawing conclusions.

**Firstly, let us have a look at the group distribution of clustring**
"""

#Plotting countplot of clusters
pal = ["#682F2F","#B9C0C9", "#9F8A78","#F3AB60"]
pl = sns.countplot(x=data["Clusters"], palette= pal)
pl.set_title("Distribution Of The Clusters")
plt.show()

"""The clusters seem to be fairly distributed.


"""

pl=sns.scatterplot(data=data,x=data['Spent'],y=data['Income'],hue=data['Clusters'],palette=pal)
pl.set_title("Cluster's Profile Based On Income And Spending")
plt.legend()
plt.show()

"""Income vs spending plot shows the clusters pattern


group 0: high spending & average income

group 1: high spending & high income

group 2: low spending & low income

group 3: high spending & low income

Next, I will be looking at the detailed distribution of clusters as per the various products in the data. Namely: Wines, Fruits, Meat, Fish, Sweets and Gold
"""

plt.figure()
pl=sns.swarmplot(x=data['Clusters'],y=data['Spent'], color= "#CBEDDD", alpha=0.5 )
pl=sns.boxenplot(x=data["Clusters"], y=data["Spent"], palette=pal)
plt.show()

"""From the above plot, it can be clearly seen that cluster 2 is our biggest set of customers closely followed by cluster 0. We can explore what each cluster is spending on for the targeted marketing strategies.

Let us next explore how did our campaigns do in the past.
"""

#Creating a feature to get a sum of accepted promotions
data["Total_Promos"] = data["AcceptedCmp1"]+ data["AcceptedCmp2"]+ data["AcceptedCmp3"]+ data["AcceptedCmp4"]+ data["AcceptedCmp5"]
#Plotting count of total campaign accepted.
plt.figure()
pl = sns.countplot(x=data["Total_Promos"],hue=data["Clusters"], palette= pal)
pl.set_title("Count Of Promotion Accepted")
pl.set_xlabel("Number Of Total Accepted Promotions")
plt.show()

"""There has not been an overwhelming response to the campaigns so far. Very few participants overall. Moreover, no one part take in all 5 of them. Perhaps better-targeted and well-planned campaigns are required to boost sales."""

#Plotting the number of deals purchased
plt.figure()
pl=sns.boxenplot(y=data["NumDealsPurchases"],x=data["Clusters"], palette= pal)
pl.set_title("Number of Deals Purchased")
plt.show()

"""Unlike campaigns, the deals offered did well. It has best outcome with cluster 0 and cluster 3. However, our star customers cluster 2 are not much into the deals. Nothing seems to attract cluster 1 overwhelmingly

#PROFILING

Now that we have formed the clusters and looked at their purchasing habits. Let us see who all are there in these clusters. For that, we will be profiling the clusters formed and come to a conclusion about who is our star customer and who needs more attention from the retail store's marketing team.

To decide that I will be plotting some of the features that are indicative of the customer's personal traits in light of the cluster they are in. On the basis of the outcomes, I will be arriving at the conclusions.
"""

Personal = [ "Kidhome","Teenhome","Customer_For", "Age", "Children", "Family_Size", "Is_Parent", "Education","Living_With"]

for i in Personal:
    plt.figure()
    sns.jointplot(x=data[i], y=data["Spent"], hue =data["Clusters"], kind="kde", palette=pal)
    plt.show()

"""# Graph's Explaination

## 1. Kidhome vs Spent

* **Clusters 0 & 2:** High spending when Kidhome = 0; spending drops as kids increase
* **Clusters 1 & 3:** Low spending regardless of Kidhome
* **Conclusion:** Fewer kids → higher spending, mainly in high-value clusters

---

## 2. Teenhome vs Spent

* **Clusters 0 & 2:** Moderate spending with 0–1 teens, lower with more teens
* **Clusters 1 & 3:** Consistently low spending
* **Conclusion:** Teen presence reduces discretionary spending in premium clusters

---

## 3. Children vs Spent

* **Clusters 0 & 2:** Highest spending with 0–1 child, sharp decline with more children
* **Clusters 1 & 3:** Minimal change, low spending
* **Conclusion:** Larger families spend less, especially in high-value clusters

---

## 4. Family_Size vs Spent

* **Clusters 0 & 2:** Small families (2–3 members) spend the most
* **Clusters 1 & 3:** Larger family sizes with lower spending
* **Conclusion:** Compact households are more profitable

---

## 5. Is_Parent vs Spent

* **Clusters 0 & 2:** Non-parents spend significantly more
* **Clusters 1 & 3:** Low spending regardless of parenting status
* **Conclusion:** Parenthood reduces spending mainly in premium clusters

---

## 6. Living_With vs Spent

* **Clusters 0 & 2:** Higher spending when living with someone
* **Clusters 1 & 3:** Slight or no change
* **Conclusion:** Shared households increase spending in high-value segments

---

## 7. Education vs Spent

* **Clusters 0 & 2:** Spending rises with higher education
* **Clusters 1 & 3:** Education has little impact
* **Conclusion:** Education predicts spending mainly for premium customers

---

## 8. Age vs Spent

* **Clusters 0 & 2:** Peak spending in middle age
* **Clusters 1 & 3:** Low spending across ages
* **Conclusion:** Middle-aged customers drive revenue in top clusters

---

## 9. Customer_For vs Spent

* **Clusters 0 & 2:** Long-term customers spend significantly more
* **Clusters 1 & 3:** Remain low spenders even over time
* **Conclusion:** Loyalty strongly increases spending in high-value clusters

---

> Across all personal traits, Clusters 0 and 2 consistently represent high-value customers whose spending is influenced by education, family structure, age, and loyalty, while Clusters 1 and 3 remain low-spending and less sensitive to personal traits.

## Final  conclusion


Clusters 0 and 2 represent the high-value customer segments, where spending is strongly influenced by personal traits such as higher education, smaller family size, living with a partner, middle age, and long-term association with the company; these customers should be targeted with premium products, loyalty programs, personalized recommendations, VIP memberships, and upselling strategies, focusing more on retention and experience rather than discounts. Cluster 2, being the most profitable, is ideal for elite services, early product access, and high-end bundles, while Cluster 0 benefits from subscription models and rewards.

In contrast, Clusters 1 and 3 are low-spending segments that show limited sensitivity to personal traits, where Cluster 1 consists of disengaged or price-sensitive customers requiring discounts, reactivation campaigns, and low-cost offerings, and Cluster 3 includes occasional or budget buyers who respond better to seasonal offers, bundled deals, and price-based promotions aimed at increasing repeat purchases and gradual upselling.

In this project, I performed unsupervised clustering. I did use dimensionality reduction followed by agglomerative clustering. I came up with 4 clusters and further used them in profiling customers in clusters according to their family structures and income/spending. This can be used in planning better marketing strategies.
"""

